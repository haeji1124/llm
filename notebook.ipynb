{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\STUDY\\Programing\\Python\\gpt\\3\\env\\lib\\site-packages\\langchain\\llms\\openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\STUDY\\Programing\\Python\\gpt\\3\\env\\lib\\site-packages\\langchain\\llms\\openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('태양계에는 8개의 행성이 있습니다. 수성, 금성, 지구, 화성, 목성, 토성, 천왕성, 명왕성입니다. 하지만 명왕성은 왜도에 위치하고 있으며 행성으로 인정받지 못하고 있습니다.',\n",
       " '태양계에는 8개의 행성이 있습니다. 이들은 수성, 금성, 지구, 화성, 목성, 토성, 천왕성, 해왕성입니다. 추가로, 2006년에 소행성인 명왕성이 행성이 아니라는 결정을 했지만, 여전히 일반적으로 행성으로 간주됩니다.')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-1106\")\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "a = llm.predict(\"행성은 몇개가 있나요?\")  # davinci\n",
    "b = chat.predict(\"행성은 몇개가 있나요?\") # gpt-3.5-turbo\n",
    "\n",
    "a,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Mi dispiace, ma 저는 이탈리아어로만 대답할 수 있습니다. 멕시코와 태국 사이의 거리는 대략 15,000km 정도입니다. 제 이름은 파올로입니다.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1 # 창의성을 나타내는 지표. 높을수록 창의적\n",
    ")\n",
    "\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "# AIMessage : AI에 의해 보내지는거\n",
    "# SystemMessage : 우리가 LLM에 설정들을 제공하기 위한 Message\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"당신은 지리학자입니다. 당신은 오직 이탈리아어로 대답합니다.\"),\n",
    "    # SystemMessage(content=\"you are a geography expert. And you only reply in italian.\"),\n",
    "    AIMessage(content=\"ciao, mi chiamo Paolo\"),\n",
    "    HumanMessage(content=\"멕시코와 태국 사이의 거리는 어떻게 되나요? 그리고 당신의 이름은 무엇인가요?\")\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국과 미국 사이의 거리는 직선거리로 약 10,000km 정도입니다. 하지만 실제로 이동할 때에는 비행기나 배 등을 이용하여 이동해야 하므로 실제 이동 거리는 더 길어질 수 있습니다. 대한민국과 미국은 태평양에 위치해 있기 때문에 이동 거리가 상당히 멀다고 할 수 있습니다.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "# ChatPromptTemplate : template을 message로 부터 만든다.\n",
    "# PromptTemplate : string 으로 template을 만든다.\n",
    "\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # 답변 생성하는 과정 볼 수 있게\n",
    "\n",
    "template = PromptTemplate.from_template(\"{국가1}과 {국가2}의 거리가 어떻게 되나요?\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0.1 # 창의성을 나타내는 지표. 높을수록 창의적\n",
    ")\n",
    "\n",
    "prompt = template.format(국가1='대한민국', 국가2='미국')\n",
    "\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! Mi chiamo Paolo. La distanza tra la Corea del Sud e gli Stati Uniti dipende dalla città di partenza e di arrivo. Ad esempio, la distanza tra Seoul e New York City è di circa 11,000 chilometri.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 지리학자입니다. 당신은 오직 {언어}로 대답합니다.\"),\n",
    "    (\"ai\", \"ciao, mi chiamo {이름}\"),\n",
    "    (\"human\", \"{국가1}과 {국가2}의 거리가 어떻게 되나요? 그리고 당신의 이름은 무엇인가요?\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    언어 = \"italian\",\n",
    "    이름 = \"pauolo\",\n",
    "    국가1 = \"대한민국\",\n",
    "    국가2 = \"미국\"\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 항상 텍스트로 대답하는 llm의 대답을 원하는 형태로 만들어보자\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(\"Hello, world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='빨강, 주황, 노랑, 초록, 파랑, 남색, 보라, 핑크, 갈색, 회색')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.Do NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items=10, \n",
    "    question=\"색상이 뭐가 있을까?\"\n",
    ")\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['빨강', '주황', '노랑', '초록', '파랑', '남색', '보라', '핑크', '갈색', '회색']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['피카츄', '파이리', '꼬부기', '이상해씨', '꼬마돌']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({\n",
    "    \"max_items\":5,\n",
    "    \"question\":\"포켓몬 종류 말해줘\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일반 요리사의 레시피:\n",
      "인도 요리는 매우 다양하고 맛있는데, 그 중에서도 쉽게 만들 수 있는 \"인도식 치킨 커리\" 레시피를 알려드리겠습니다.\n",
      "\n",
      "인도식 치킨 커리 레시피:\n",
      "\n",
      "재료:\n",
      "- 닭가슴살 500g\n",
      "- 양파 1개\n",
      "- 마늘 3쪽\n",
      "- 생강 1조각 (약 2cm)\n",
      "- 토마토 2개\n",
      "- 코리앤더 가루 1큰술\n",
      "- 케이프 1큰술\n",
      "- 고추가루 1큰술\n",
      "- 소금 약간\n",
      "- 식용유 2큰술\n",
      "- 물 1컵\n",
      "\n",
      "1. 양파, 마늘, 생강을 다듬어 다져놓습니다.\n",
      "2. 닭가슴살을 한입 크기로 잘라놓습니다.\n",
      "3. 팬에 식용유를 두르고 양파, 마늘, 생강을 볶아줍니다.\n",
      "4. 양파가 투명해지면 닭가슴살을 넣고 볶아줍니다.\n",
      "5. 토마토를 다져서 넣고 볶아줍니다.\n",
      "6. 코리앤더 가루, 케이프, 고추가루, 소금을 넣고 볶아줍니다.\n",
      "7. 물을 넣고 뚜껑을 덮어 약불에서 15분간 끓입니다.\n",
      "8. 뚜껑을 열고 끓여 만든 소스가 뿌옇게 될 때까지 끓여줍니다.\n",
      "9. 밥이나 나안을 곁들여 인도식 치킨 커리를 즐기세요.\n",
      "\n",
      "이렇게 간단하게 만들 수 있는 인도식 치킨 커리는 매우 맛있고 가정에서도 쉽게 만들 수 있습니다. 매운맛을 좋아하신다면 고추가루를 더 넣어 매콤하게 조리해도 좋습니다.즐겨보세요!채식주의자를 위한 대체재료로는 대체로 닭가슴살 대신 대체육이나 대체 단백질 소스를 사용할 수 있습니다. 대체육 상품은 대부분 슈퍼마켓이나 온라인에서 구매할 수 있습니다. 대체 단백질 소스로는 대체로 두부, 콩, 혹은 채소 단백질이 들어간 대체육을 사용할 수 있습니다. 이러한 대체재료를 사용하여 전통적인 인도식 치킨 커리 레시피를 준비하면 채식주의자도 즐길 수 있는 맛있는 요리를 즐길 수 있습니다.\n",
      "채식주의자 요리사의 레시피:\n",
      "채식주의자를 위한 대체재료로는 대체로 닭가슴살 대신 대체육이나 대체 단백질 소스를 사용할 수 있습니다. 대체육 상품은 대부분 슈퍼마켓이나 온라인에서 구매할 수 있습니다. 대체 단백질 소스로는 대체로 두부, 콩, 혹은 채소 단백질이 들어간 대체육을 사용할 수 있습니다. 이러한 대체재료를 사용하여 전통적인 인도식 치킨 커리 레시피를 준비하면 채식주의자도 즐길 수 있는 맛있는 요리를 즐길 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "# 스트리밍 콜백 핸들러 생성\n",
    "streaming_handler = StreamingStdOutCallbackHandler()\n",
    "\n",
    "# 콜백 매니저 생성\n",
    "callback_manager = CallbackManager([streaming_handler])\n",
    "\n",
    "# ChatOpenAI 모델 초기화\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    "    callback_manager=callback_manager,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# 첫 번째 쉐프 프롬프트\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 월드클래스 국제 요리사입니다. 당신은 어떤 종류의 요리든 쉽게 구할 수 있는 재료로 따라하기 쉬운 레시피를 만들어 줍니다.\"),\n",
    "    (\"human\", \"나는 {cuisine}를 요리하기 원합니다.\")\n",
    "])\n",
    "\n",
    "# 채식주의자 쉐프 프롬프트\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 채식주의자를 위한 세프입니다. 전통적인 채식주의자용 레시피에 특화되어 있습니다. 당신은 대체 재료를 찾고, 준비하는 방법에 대해 설명합니다. 기존 레시피를 너무 많이 변경해서는 안됩니다. 만약 다른 대체품이 없다면 그냥 레시피를 모른다고 말하세요\"),\n",
    "    (\"human\", \"{recipe}\")\n",
    "])\n",
    "\n",
    "# 체인 구성\n",
    "chef_chain = chef_prompt | chat | StrOutputParser()\n",
    "veg_chain = veg_chef_prompt | chat | StrOutputParser()\n",
    "\n",
    "# 최종 체인 구성\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "# 체인 실행\n",
    "print(\"일반 요리사의 레시피:\")\n",
    "result = final_chain.invoke({\"cuisine\": \"indian\"})\n",
    "\n",
    "print(\"\\n채식주의자 요리사의 레시피:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프랑스는 유럽 대륙의 서쪽에 위치한 나라로 파리를 수도로 하고 있습니다. 프랑스는 세계적으로 유명한 문화와 예술의 중심지로서 유명하며, 에펠탑, 루브르 박물관, 베르사유 궁전 등 많은 유명한 관광지가 있습니다. 또한 프랑스는 와인, 치즈, 요리 등의 다양한 음식 문화로도 유명하며, 세계적으로 유명한 디자이너 브랜드들이 많이 탄생한 곳으로도 알려져 있습니다. 프랑스는 또한 유럽 연합의 주요 회원국 중 하나이며, 세계 경제에서도 중요한 역할을 하는 나라 중 하나입니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'프랑스는 유럽 대륙의 서쪽에 위치한 나라로 파리를 수도로 하고 있습니다. 프랑스는 세계적으로 유명한 문화와 예술의 중심지로서 유명하며, 에펠탑, 루브르 박물관, 베르사유 궁전 등 많은 유명한 관광지가 있습니다. 또한 프랑스는 와인, 치즈, 요리 등의 다양한 음식 문화로도 유명하며, 세계적으로 유명한 디자이너 브랜드들이 많이 탄생한 곳으로도 알려져 있습니다. 프랑스는 또한 유럽 연합의 주요 회원국 중 하나이며, 세계 경제에서도 중요한 역할을 하는 나라 중 하나입니다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, \n",
    "    streaming=True, \n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# t = PromptTemplate(\n",
    "#     template=\"{국가}의 수도는 어디인가요?\",\n",
    "#     input_variables=[\"country\"]\n",
    "# )\n",
    "\n",
    "# t.format(국가=\"미국\")\n",
    "\n",
    "chat.predict(\"프랑스에 대해 무엇을 알고 있어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Human: What do you know about France?\\n    AI: \\n        Here is what I know:\\n        Capital: Paris\\n        Language: French\\n        Food: Wine and Cheese\\n        Currency: Euro\\n        \\n\\n\\n\\n    Human: What do you know about Italy?\\n    AI: \\n        I know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        \\n\\n\\n\\n    Human: What do you know about Greece?\\n    AI: \\n        I know this:\\n        Capital: Athens\\n        Language: Greek\\n        Food: Souvlaki and Feta Cheese\\n        Currency: Euro\\n        \\n\\n\\nHuman: 프랑스에 대해 무엇을 알고 있어?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_template=\"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: {country}에 대해 무엇을 알고 있어?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"프랑스\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "미국에 대해 알고 있는 정보는 다음과 같습니다:\n",
      "수도: Washington D.C.\n",
      "언어: 영어\n",
      "음식: 핫도그, 햄버거\n",
      "통화: 미국 달러"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI: \\n미국에 대해 알고 있는 정보는 다음과 같습니다:\\n수도: Washington D.C.\\n언어: 영어\\n음식: 핫도그, 햄버거\\n통화: 미국 달러')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"미국\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 is the Korean name for the United States. Here is some information:\n",
      "        Capital: Washington, D.C.\n",
      "        Language: English\n",
      "        Food: Burgers and Hotdogs\n",
      "        Currency: US Dollar"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='미국 is the Korean name for the United States. Here is some information:\\n        Capital: Washington, D.C.\\n        Language: English\\n        Food: Burgers and Hotdogs\\n        Currency: US Dollar')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a geography expert, you give short answers\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"미국\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Human: What do you know about France?\\n    AI: \\n        Here is what I know:\\n        Capital: Paris\\n        Language: French\\n        Food: Wine and Cheese\\n        Currency: Euro\\n        \\n\\n\\n\\n    Human: What do you know about Italy?\\n    AI: \\n        I know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        \\n\\n\\n\\n    Human: What do you know about Greece?\\n    AI: \\n        I know this:\\n        Capital: Athens\\n        Language: Greek\\n        Food: Souvlaki and Feta Cheese\\n        Currency: Euro\\n        \\n\\n\\nHuman: 영국에 대해 무엇을 알고 있어?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt,\n",
    "    max_length=80\n",
    ")\n",
    "\n",
    "example_prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: {country}에 대해 무엇을 알고 있어?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"영국\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about Italy?\\nAI:\\n        I know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about 브라질?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "    \n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"브라질\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Germany'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models  import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    \\n    너는 롤 플레잉 어시스턴트야.\\n    너는 피카츄을 흉내내고 있어.\\n    \\n\\n    \\n    이것은 니가 말하는 방식의 예시야:\\n    \\n    Human: 너 어디있어?\\n    You: 피카피카 나는 부산에 있어. 삐까츄~\\n    \\n\\n    \\n    시작한다!\\n    Human: 니가 제일 좋아하는 음식이 뭐야?\\n    You: \\n    \\n    '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    너는 롤 플레잉 어시스턴트야.\n",
    "    너는 {character}을 흉내내고 있어.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    이것은 니가 말하는 방식의 예시야:\n",
    "    \n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    시작한다!\n",
    "    Human: {question}\n",
    "    You: \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(final_prompt=final, pipeline_prompts=prompts)\n",
    "\n",
    "full_prompt.format(\n",
    "    character=\"피카츄\",\n",
    "    example_question=\"너 어디있어?\",\n",
    "    example_answer=\"피카피카 나는 부산에 있어. 삐까츄~\",\n",
    "    question=\"니가 제일 좋아하는 음식이 뭐야?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피카츄! 나는 과일을 좋아해. 피카피카~"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='피카츄! 나는 과일을 좋아해. 피카피카~')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"character\":\"피카츄\",\n",
    "    \"example_question\":\"너 어디있어?\",\n",
    "    \"example_answer\":\"피카피카 나는 부산에 있어. 삐까츄~\",\n",
    "    \"question\":\"니가 제일 좋아하는 음식이 뭐야?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓인 후 파스타를 넣어서 삶습니다. 삶는 시간은 각 종류마다 다르므로 포장지에 적힌 시간을 참고하세요.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다.\\n\\n3. 볶은 양파와 마늘에 토마토 소스를 넣고 약간의 소금과 후추로 간을 해줍니다.\\n\\n4. 삶은 파스타를 물기를 제거하고 소스에 넣어 섞어줍니다.\\n\\n5. 마지막으로 파마산 치즈나 파슬리를 곁들여서 내놓으면 완성입니다.\\n\\n이 외에도 다양한 파스타 레시피가 있으니 자신이 좋아하는 재료와 소스를 활용하여 맛있는 이탈리아 파스타를 만들어보세요.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache # LLM의 응답을 저장\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ]\n",
    ")\n",
    "\n",
    "chat.predict(\"이탈리아 파스타는 어떻게 만드나요?\") #5.9s가 걸렸지만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓인 후 파스타를 넣어서 삶습니다. 삶는 시간은 각 종류마다 다르므로 포장지에 적힌 시간을 참고하세요.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다.\\n\\n3. 볶은 양파와 마늘에 토마토 소스를 넣고 약간의 소금과 후추로 간을 해줍니다.\\n\\n4. 삶은 파스타를 물기를 제거하고 소스에 넣어 섞어줍니다.\\n\\n5. 마지막으로 파마산 치즈나 파슬리를 곁들여서 내놓으면 완성입니다.\\n\\n이 외에도 다양한 파스타 레시피가 있으니 자신이 좋아하는 재료와 소스를 활용하여 맛있는 이탈리아 파스타를 만들어보세요.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"이탈리아 파스타는 어떻게 만드나요?\") # 같은 질문을 하면 0.8s 걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 이탈리아 파스타는 어떻게 만드나요?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [1ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓인 후 파스타를 넣어서 삶습니다. 삶는 시간은 각 종류마다 다르므로 포장지에 적힌 시간을 참고하세요.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다.\\n\\n3. 볶은 양파와 마늘에 토마토 소스를 넣고 약간의 소금과 후추로 간을 해줍니다.\\n\\n4. 삶은 파스타를 물기를 제거하고 소스에 넣어 섞어줍니다.\\n\\n5. 마지막으로 파마산 치즈나 파슬리를 곁들여서 내놓으면 완성입니다.\\n\\n이 외에도 다양한 파스타 레시피가 있으니 자신이 좋아하는 재료와 소스를 활용하여 맛있는 이탈리아 파스타를 만들어보세요.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓인 후 파스타를 넣어서 삶습니다. 삶는 시간은 각 종류마다 다르므로 포장지에 적힌 시간을 참고하세요.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다.\\n\\n3. 볶은 양파와 마늘에 토마토 소스를 넣고 약간의 소금과 후추로 간을 해줍니다.\\n\\n4. 삶은 파스타를 물기를 제거하고 소스에 넣어 섞어줍니다.\\n\\n5. 마지막으로 파마산 치즈나 파슬리를 곁들여서 내놓으면 완성입니다.\\n\\n이 외에도 다양한 파스타 레시피가 있으니 자신이 좋아하는 재료와 소스를 활용하여 맛있는 이탈리아 파스타를 만들어보세요.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓인 후 파스타를 넣어서 삶습니다. 삶는 시간은 각 종류마다 다르므로 포장지에 적힌 시간을 참고하세요.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다.\\n\\n3. 볶은 양파와 마늘에 토마토 소스를 넣고 약간의 소금과 후추로 간을 해줍니다.\\n\\n4. 삶은 파스타를 물기를 제거하고 소스에 넣어 섞어줍니다.\\n\\n5. 마지막으로 파마산 치즈나 파슬리를 곁들여서 내놓으면 완성입니다.\\n\\n이 외에도 다양한 파스타 레시피가 있으니 자신이 좋아하는 재료와 소스를 활용하여 맛있는 이탈리아 파스타를 만들어보세요.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache, set_debug # set_debug 모드로 출력결과가 상세히 나옴\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "chat.predict(\"이탈리아 파스타는 어떻게 만드나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 이탈리아 파스타는 어떻게 만드나요?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [6ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓는 물에 파스타를 넣어서 삶습니다. 대부분의 이탈리아 파스타는 알 데인테(알 데인테)로 삶아내는 것이 좋습니다. 삶는 시간은 각각의 파스타 종류에 따라 다를 수 있습니다.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다. 그 후 토마토 소스나 크림 소스를 넣어서 함께 볶아줍니다.\\n\\n3. 삶은 파스타를 물에 헹궈 물기를 제거한 후, 팬에 볶은 소스와 섞어줍니다. 파마산 치즈나 파슬리, 후추 등을 넣어 맛을 낸 후에는 그릇에 담아서 내놓습니다.\\n\\n4. 마지막으로, 파스타 위에 파마산 치즈를 솔솔 뿌려주고 신선한 바질 잎을 올려서 마무리합니다.\\n\\n이렇게 만들어진 이탈리아 파스타는 집에서도 쉽게 만들 수 있고 맛있게 즐길 수 있습니다.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓는 물에 파스타를 넣어서 삶습니다. 대부분의 이탈리아 파스타는 알 데인테(알 데인테)로 삶아내는 것이 좋습니다. 삶는 시간은 각각의 파스타 종류에 따라 다를 수 있습니다.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다. 그 후 토마토 소스나 크림 소스를 넣어서 함께 볶아줍니다.\\n\\n3. 삶은 파스타를 물에 헹궈 물기를 제거한 후, 팬에 볶은 소스와 섞어줍니다. 파마산 치즈나 파슬리, 후추 등을 넣어 맛을 낸 후에는 그릇에 담아서 내놓습니다.\\n\\n4. 마지막으로, 파스타 위에 파마산 치즈를 솔솔 뿌려주고 신선한 바질 잎을 올려서 마무리합니다.\\n\\n이렇게 만들어진 이탈리아 파스타는 집에서도 쉽게 만들 수 있고 맛있게 즐길 수 있습니다.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓는 물에 파스타를 넣어서 삶습니다. 대부분의 이탈리아 파스타는 알 데인테(알 데인테)로 삶아내는 것이 좋습니다. 삶는 시간은 각각의 파스타 종류에 따라 다를 수 있습니다.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다. 그 후 토마토 소스나 크림 소스를 넣어서 함께 볶아줍니다.\\n\\n3. 삶은 파스타를 물에 헹궈 물기를 제거한 후, 팬에 볶은 소스와 섞어줍니다. 파마산 치즈나 파슬리, 후추 등을 넣어 맛을 낸 후에는 그릇에 담아서 내놓습니다.\\n\\n4. 마지막으로, 파스타 위에 파마산 치즈를 솔솔 뿌려주고 신선한 바질 잎을 올려서 마무리합니다.\\n\\n이렇게 만들어진 이탈리아 파스타는 집에서도 쉽게 만들 수 있고 맛있게 즐길 수 있습니다.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import InMemoryCache, SQLiteCache # SQLiteCache : 데이터베이스에 캐싱\n",
    "\n",
    "set_llm_cache(SQLiteCache())\n",
    "\n",
    "chat.predict(\"이탈리아 파스타는 어떻게 만드나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 이탈리아 파스타는 어떻게 만드나요?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [4ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓는 물에 파스타를 넣어서 삶습니다. 대부분의 이탈리아 파스타는 알 데인테(알 데인테)로 삶아내는 것이 좋습니다. 삶는 시간은 각각의 파스타 종류에 따라 다를 수 있습니다.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다. 그 후 토마토 소스나 크림 소스를 넣어서 함께 볶아줍니다.\\n\\n3. 삶은 파스타를 물에 헹궈 물기를 제거한 후, 팬에 볶은 소스와 섞어줍니다. 파마산 치즈나 파슬리, 후추 등을 넣어 맛을 낸 후에는 그릇에 담아서 내놓습니다.\\n\\n4. 마지막으로, 파스타 위에 파마산 치즈를 솔솔 뿌려주고 신선한 바질 잎을 올려서 마무리합니다.\\n\\n이렇게 만들어진 이탈리아 파스타는 집에서도 쉽게 만들 수 있고 맛있게 즐길 수 있습니다.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓는 물에 파스타를 넣어서 삶습니다. 대부분의 이탈리아 파스타는 알 데인테(알 데인테)로 삶아내는 것이 좋습니다. 삶는 시간은 각각의 파스타 종류에 따라 다를 수 있습니다.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다. 그 후 토마토 소스나 크림 소스를 넣어서 함께 볶아줍니다.\\n\\n3. 삶은 파스타를 물에 헹궈 물기를 제거한 후, 팬에 볶은 소스와 섞어줍니다. 파마산 치즈나 파슬리, 후추 등을 넣어 맛을 낸 후에는 그릇에 담아서 내놓습니다.\\n\\n4. 마지막으로, 파스타 위에 파마산 치즈를 솔솔 뿌려주고 신선한 바질 잎을 올려서 마무리합니다.\\n\\n이렇게 만들어진 이탈리아 파스타는 집에서도 쉽게 만들 수 있고 맛있게 즐길 수 있습니다.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이탈리아 파스타를 만드는 방법은 다양하지만, 전통적인 방법은 다음과 같습니다:\\n\\n1. 물에 소금을 넣고 끓는 물에 파스타를 넣어서 삶습니다. 대부분의 이탈리아 파스타는 알 데인테(알 데인테)로 삶아내는 것이 좋습니다. 삶는 시간은 각각의 파스타 종류에 따라 다를 수 있습니다.\\n\\n2. 파스타를 삶는 동안, 별도의 팬에 올리브 오일을 데워 양파와 마늘을 볶습니다. 그 후 토마토 소스나 크림 소스를 넣어서 함께 볶아줍니다.\\n\\n3. 삶은 파스타를 물에 헹궈 물기를 제거한 후, 팬에 볶은 소스와 섞어줍니다. 파마산 치즈나 파슬리, 후추 등을 넣어 맛을 낸 후에는 그릇에 담아서 내놓습니다.\\n\\n4. 마지막으로, 파스타 위에 파마산 치즈를 솔솔 뿌려주고 신선한 바질 잎을 올려서 마무리합니다.\\n\\n이렇게 만들어진 이탈리아 파스타는 집에서도 쉽게 만들 수 있고 맛있게 즐길 수 있습니다.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"이탈리아 파스타는 어떻게 만드나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 소주는 어떻게 만들어?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [3ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"소주는 주로 쌀을 사용하여 만들어집니다. 먼저 쌀을 씻고 증기를 통해 삶아서 쌀의 전분을 녹인 후 발효시켜 술맥주를 만듭니다. 그 후 술맥주를 증류하여 술을 추출하고 여러 번 걸쳐 여과하여 맑은 술을 얻습니다. 이후 술을 숙성시켜 부드럽고 깔끔한 맛을 내는 과정을 거쳐 완성된 소주가 됩니다.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"소주는 주로 쌀을 사용하여 만들어집니다. 먼저 쌀을 씻고 증기를 통해 삶아서 쌀의 전분을 녹인 후 발효시켜 술맥주를 만듭니다. 그 후 술맥주를 증류하여 술을 추출하고 여러 번 걸쳐 여과하여 맑은 술을 얻습니다. 이후 술을 숙성시켜 부드럽고 깔끔한 맛을 내는 과정을 거쳐 완성된 소주가 됩니다.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback # 사용량을(비용) 볼 수 있다.\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    chat.predict(\"소주는 어떻게 만들어?\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 김치찌개는 어떻게 만들어?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [2ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"김치찌개를 만드는 방법은 다음과 같습니다:\\n\\n재료:\\n- 김치 2컵\\n- 물 4컵\\n- 돼지고기 혹은 소고기 200g\\n- 두부 1/2모\\n- 대파 1대\\n- 양파 1/2개\\n- 청양고추 1개\\n- 고춧가루 1큰술\\n- 고추장 1큰술\\n- 간장 1큰술\\n- 다진 마늘 1큰술\\n- 소금, 후추 약간\\n\\n1. 냄비에 물을 붓고 김치를 넣어 끓인다.\\n2. 돼지고기나 소고기를 넣고 끓인다.\\n3. 대파, 양파, 청양고추를 썰어 넣고 끓인다.\\n4. 고춧가루, 고추장, 간장, 다진 마늘을 넣고 끓인다.\\n5. 두부를 넣고 끓인다.\\n6. 소금과 후추로 간을 맞추고 끓인다.\\n\\n맛있는 김치찌개가 완성되었습니다. 맛있게 드세요!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"김치찌개를 만드는 방법은 다음과 같습니다:\\n\\n재료:\\n- 김치 2컵\\n- 물 4컵\\n- 돼지고기 혹은 소고기 200g\\n- 두부 1/2모\\n- 대파 1대\\n- 양파 1/2개\\n- 청양고추 1개\\n- 고춧가루 1큰술\\n- 고추장 1큰술\\n- 간장 1큰술\\n- 다진 마늘 1큰술\\n- 소금, 후추 약간\\n\\n1. 냄비에 물을 붓고 김치를 넣어 끓인다.\\n2. 돼지고기나 소고기를 넣고 끓인다.\\n3. 대파, 양파, 청양고추를 썰어 넣고 끓인다.\\n4. 고춧가루, 고추장, 간장, 다진 마늘을 넣고 끓인다.\\n5. 두부를 넣고 끓인다.\\n6. 소금과 후추로 간을 맞추고 끓인다.\\n\\n맛있는 김치찌개가 완성되었습니다. 맛있게 드세요!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 된장찌개는 어떻게 만들어?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [2ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"된장찌개는 한국의 전통 음식으로, 간단하면서도 맛있는 요리입니다. 만드는 방법은 다음과 같습니다:\\n\\n재료:\\n- 된장 2큰술\\n- 물 4컵\\n- 두부 1/2모\\n- 양파 1/2개\\n- 대파 1대\\n- 청양고추 1개\\n- 마늘 2쪽\\n- 고추장 1큰술\\n- 소금 약간\\n- 참기름 1큰술\\n\\n1. 대파와 양파는 얇게 채 썰고, 청양고추는 얇게 채썰어 준비합니다.\\n2. 두부는 2cm 크기로 잘라 준비합니다.\\n3. 냄비에 된장과 물을 넣고 섞어줍니다.\\n4. 불을 켜고 된장물에 대파, 양파, 청양고추, 마늘을 넣고 끓여줍니다.\\n5. 두부와 고추장을 넣고 끓여줍니다.\\n6. 소금으로 간을 해주고 참기름을 넣어 마무리합니다.\\n\\n된장찌개는 밥과 함께 먹으면 더욱 맛있습니다. 매운 음식을 좋아하신다면 고추장이나 고추가루를 추가하여 매운맛을 더해도 좋습니다.맛있게 즐기세요!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"된장찌개는 한국의 전통 음식으로, 간단하면서도 맛있는 요리입니다. 만드는 방법은 다음과 같습니다:\\n\\n재료:\\n- 된장 2큰술\\n- 물 4컵\\n- 두부 1/2모\\n- 양파 1/2개\\n- 대파 1대\\n- 청양고추 1개\\n- 마늘 2쪽\\n- 고추장 1큰술\\n- 소금 약간\\n- 참기름 1큰술\\n\\n1. 대파와 양파는 얇게 채 썰고, 청양고추는 얇게 채썰어 준비합니다.\\n2. 두부는 2cm 크기로 잘라 준비합니다.\\n3. 냄비에 된장과 물을 넣고 섞어줍니다.\\n4. 불을 켜고 된장물에 대파, 양파, 청양고추, 마늘을 넣고 끓여줍니다.\\n5. 두부와 고추장을 넣고 끓여줍니다.\\n6. 소금으로 간을 해주고 참기름을 넣어 마무리합니다.\\n\\n된장찌개는 밥과 함께 먹으면 더욱 맛있습니다. 매운 음식을 좋아하신다면 고추장이나 고추가루를 추가하여 매운맛을 더해도 좋습니다.맛있게 즐기세요!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "김치찌개를 만드는 방법은 다음과 같습니다:\n",
      "\n",
      "재료:\n",
      "- 김치 2컵\n",
      "- 물 4컵\n",
      "- 돼지고기 혹은 소고기 200g\n",
      "- 두부 1/2모\n",
      "- 대파 1대\n",
      "- 양파 1/2개\n",
      "- 청양고추 1개\n",
      "- 고춧가루 1큰술\n",
      "- 고추장 1큰술\n",
      "- 간장 1큰술\n",
      "- 다진 마늘 1큰술\n",
      "- 소금, 후추 약간\n",
      "\n",
      "1. 냄비에 물을 붓고 김치를 넣어 끓인다.\n",
      "2. 돼지고기나 소고기를 넣고 끓인다.\n",
      "3. 대파, 양파, 청양고추를 썰어 넣고 끓인다.\n",
      "4. 고춧가루, 고추장, 간장, 다진 마늘을 넣고 끓인다.\n",
      "5. 두부를 넣고 끓인다.\n",
      "6. 소금과 후추로 간을 맞추고 끓인다.\n",
      "\n",
      "맛있는 김치찌개가 완성되었습니다. 맛있게 드세요! \n",
      "\n",
      "된장찌개는 한국의 전통 음식으로, 간단하면서도 맛있는 요리입니다. 만드는 방법은 다음과 같습니다:\n",
      "\n",
      "재료:\n",
      "- 된장 2큰술\n",
      "- 물 4컵\n",
      "- 두부 1/2모\n",
      "- 양파 1/2개\n",
      "- 대파 1대\n",
      "- 청양고추 1개\n",
      "- 마늘 2쪽\n",
      "- 고추장 1큰술\n",
      "- 소금 약간\n",
      "- 참기름 1큰술\n",
      "\n",
      "1. 대파와 양파는 얇게 채 썰고, 청양고추는 얇게 채썰어 준비합니다.\n",
      "2. 두부는 2cm 크기로 잘라 준비합니다.\n",
      "3. 냄비에 된장과 물을 넣고 섞어줍니다.\n",
      "4. 불을 켜고 된장물에 대파, 양파, 청양고추, 마늘을 넣고 끓여줍니다.\n",
      "5. 두부와 고추장을 넣고 끓여줍니다.\n",
      "6. 소금으로 간을 해주고 참기름을 넣어 마무리합니다.\n",
      "\n",
      "된장찌개는 밥과 함께 먹으면 더욱 맛있습니다. 매운 음식을 좋아하신다면 고추장이나 고추가루를 추가하여 매운맛을 더해도 좋습니다.맛있게 즐기세요! \n",
      "\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"김치찌개는 어떻게 만들어?\")\n",
    "    b = chat.predict(\"된장찌개는 어떻게 만들어?\")\n",
    "    print(a, \"\\n\")\n",
    "    print(b, \"\\n\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi!\\nAI: How are you?'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory \n",
    "# llm은 대화를 저장을 못해. 그래서 문맥을 기억을 못해서 대화하는 기분이 안 날 수 있음.\n",
    "# 그래서 전체대화를 저장해서 계속 넘겨주는 방식으로 써봄. 굉장히 비효율적\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# 텍스트 자동완성할때 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True) # chat model을 사용하고 싶을때 return_message True\n",
    "\n",
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory \n",
    "# 특정 부분만 저장하는 메모리. 예를들어 최근 5개만 저장. 5개가 넘어가면 제일 오래된 것을 날린다.\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4 # 몇개 저장할건지\n",
    ")\n",
    "\n",
    "def add_msg(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_msg(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_msg(2,2)\n",
    "add_msg(3,3)\n",
    "add_msg(4,4)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_msg(5,5)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"\",\n",
      "  \"new_lines\": \"Human: 안녕, 나는 해쟈야. 나는 대한민국에 살고 있어.\\nAI: 와 그거 멋지다!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: 안녕, 나는 해쟈야. 나는 대한민국에 살고 있어.\\nAI: 와 그거 멋지다!\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [3ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human greets the AI in Korean and introduces themselves as Haejya living in South Korea. The AI responds positively to the introduction.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human greets the AI in Korean and introduces themselves as Haejya living in South Korea. The AI responds positively to the introduction.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The human greets the AI in Korean and introduces themselves as Haejya living in South Korea. The AI responds positively to the introduction.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory # 대화를 요약해서 저장한다.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_msg(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_msg(\"안녕, 나는 해쟈야. 나는 대한민국에 살고 있어.\", \"와 그거 멋지다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"The human greets the AI in Korean and introduces themselves as Haejya living in South Korea. The AI responds positively to the introduction.\",\n",
      "  \"new_lines\": \"Human: 한국은 매우 아름다워\\nAI: 와 나도 가고 싶다!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\nThe human greets the AI in Korean and introduces themselves as Haejya living in South Korea. The AI responds positively to the introduction.\\n\\nNew lines of conversation:\\nHuman: 한국은 매우 아름다워\\nAI: 와 나도 가고 싶다!\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [2ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human greets the AI in Korean and introduces themselves as Haejya living in South Korea. The AI responds positively to the introduction and expresses a desire to visit Korea as well.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human greets the AI in Korean and introduces themselves as Haejya living in South Korea. The AI responds positively to the introduction and expresses a desire to visit Korea as well.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The human greets the AI in Korean and introduces themselves as Haejya living in South Korea. The AI responds positively to the introduction and expresses a desire to visit Korea as well.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "add_msg(\"한국은 매우 아름다워\", \"와 나도 가고 싶다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human greets the AI in Korean and introduces themselves as Haejya living in South Korea. The AI responds positively to the introduction and expresses a desire to visit Korea as well.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory \n",
    "# 메모리에 보내온 메시지의 수를 저장\n",
    "# limit에 다다른 순간에 그냥 잊어버리는 것 대신 오래된 메시지들을 요약한다.\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150, # 메시지가 요약되기 전에 가능한 메시지 토큰의 최대수\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!')]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"south korea is so pretty\", \"I wish I could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='south korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!')]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='south korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"How far is Korea from Argentina\", \"I don't know! Super far!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"summary\": \"\",\n",
      "  \"new_lines\": \"Human: Hi I'm Nicolas, I live in South Korea\\nAI: Wow that is so cool!\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: Hi I'm Nicolas, I live in South Korea\\nAI: Wow that is so cool!\\n\\nNew summary:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:ChatOpenAI] [976ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human introduces themselves as Nicolas and mentions they live in South Korea. The AI responds with enthusiasm, calling it cool.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human introduces themselves as Nicolas and mentions they live in South Korea. The AI responds with enthusiasm, calling it cool.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 155,\n",
      "      \"completion_tokens\": 24,\n",
      "      \"total_tokens\": 179\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [978ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The human introduces themselves as Nicolas and mentions they live in South Korea. The AI responds with enthusiasm, calling it cool.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "add_message(\"How far is Brazil from Argentina\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Nicolas and mentions they live in South Korea. The AI responds with enthusiasm, calling it cool.'),\n",
       "  HumanMessage(content='south korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "# 대화 중의 엔티티의 knowledge graph를 만든다.\n",
    "# 가장 중요한 것들만 뽑아내는 요약본\n",
    "# knowledge graph에서 히스토리를 가지고 오지 않고 엔티티를 가지고 온다.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea.')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea. Nicolas likes kimchi.')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mMy name in haeji\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Haeji'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True, # chain에게 전달하면 chain을 실행할 때 chain의 프롬프트 로그들 확인 가능\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name in haeji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "', the capital city of South Korea. It is a vibrant and bustling metropolis with a rich history and culture. There are so many things to see and do in Seoul, from exploring ancient palaces and temples to shopping in trendy neighborhoods and enjoying delicious Korean cuisine. I love living in Seoul because there is always something new and exciting to discover.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human reveals their name is Haeji to the AI, who simply responds with \"Haeji.\"\\nHuman: I live in Seoul\\nAI: , the capital city of South Korea. It is a vibrant and bustling metropolis with a rich history and culture. There are so many things to see and do in Seoul, from exploring ancient palaces and temples to shopping in trendy neighborhoods and enjoying delicious Korean cuisine. I love living in Seoul because there is always something new and exciting to discover.\\nHuman: What is my name?\\nAI: I\\'m sorry, I do not have access to personal information such as your name.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) \n",
    "# 메모리는 업데이트 되고 있는데, 프롬프트에 전달되어야하는 대화 내용이 LLM에게 전달되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is Nico\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know! How can I assist you with information or tasks related to Seoul?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to know! How can I assist you with information or tasks related to Seoul?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder \n",
    "# MessagePlaceholder를 통해 대화 기록을 promptfh 넘겨준다.\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")\n",
    "\n",
    "chain.predict(question=\"I live in Seoul\")\n",
    "\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d195b87c65532a9b0163862d9bb1aed47fdee88c10b42f49eb8d2e192227d46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
